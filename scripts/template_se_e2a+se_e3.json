{
    "loss": {
        "start_pref_f": 1000,
        "start_pref_e": 0.001,
        "limit_pref_e": 1,
        "limit_pref_v": 0.1,
        "start_pref_v": 100,
        "limit_pref_f": 1,
        "_comment": " that's all"
    },
    "model": {
        "type_map": [
            "N"
        ],
        
  "descriptor": {
    "type": "hybrid",
    "list": [
      {
        "type": "se_e2_a",
        "sel": "auto",
        "rcut_smth": 2.0,
        "rcut": 6.0,
        "neuron": [20, 40, 80],
        "resnet_dt": false,
        "axis_neuron": 16,
        "seed": 1830526208,
        "set_davg_zero": true
      },
      {
        "type": "se_e3",
        "sel": "auto",
        "rcut_smth": 2.0,
        "rcut": 4.0,
        "neuron": [4, 8, 16],
        "resnet_dt": false,
        "seed": 1,
        "set_davg_zero": true
      }
    ]
    },
        "_comment": " that's all",
        "fitting_net": {
            "neuron": [
                240,
                240,
                240
            ],
            "atom_ener" : [0, 0],   
            "resnet_dt": true,
            "seed": 18328,
            "_comment": " that's all"
        }
    },
    "training": {
        "save_ckpt": "model.ckpt",
        "disp_training": true,
        "disp_file": "lcurve.out",
        "numb_steps": 250000,
        "disp_freq": 10,
        "time_training": true,
        "seed": 13824,
        "profiling": false,
        "numb_test": 4,
        "save_freq": 50,
        "_comment": "that's all",
        "profiling_file": "timeline.json",
        "training_data": {
            "set_prefix": "set",
            "systems": [],
            "batch_size": "auto"
        },
         "validation_data":{
            "set_prefix": "set",
            "systems":		[],
            "batch_size":	"auto",
            "numb_btch":	1,
            "_comment":		"that's all"
        }

    },
    "learning_rate": {
        "stop_lr": 3.0e-8,
        "_comment": "that's all",
        "decay_steps": 2000,
        "start_lr": 0.001        
    },
    "_comment": "that's all"
}